# -*- coding: utf-8 -*-
"""Food_Recommendation_Powered_by_Multimodal_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10qumn_2CTd53ceMo1HRqske-dUaU0u4D

**1. How your chain creatively supports product marketing or personalization**
The chain creatively supports product marketing and personalization by tailoring food recommendations to the user's specific preferences. For example, when a user queries "some of the spiciest dishes," the model generates personalized suggestions based on that input, such as spicy Thai curry, Vindaloo, or Sichuan Hot Pot. This direct approach helps customers feel understood and offers them the most relevant products, making them more likely to engage. Additionally, the inclusion of images of each dish alongside detailed descriptions enhances the visual appeal and encourages the customer to explore more, turning a simple query into an interactive, personalized experience. This approach makes the interaction more dynamic and conversational, which can lead to higher engagement and conversion rates.



**2. Rationale for design choices, especially in prompt formatting and chaining**
The design choices behind the prompt formatting and chaining process are centered around maximizing relevance and user engagement. By keeping the prompt clear and focused on the user's query (e.g., "some of the spiciest dishes"), the system ensures that the response directly addresses the user's needs without extraneous information. The chain is structured to first interpret the user's query, then retrieve relevant dishes, and finally format the response to be user-friendly and engaging.

Chaining allows each component of the process (query interpretation, document retrieval, and response formatting) to function independently but in a cohesive manner. This modular design ensures that the system remains efficient, accurate, and easily extendable if additional data sources or processes need to be added later. For example, after retrieving relevant dishes, the system formats the content with descriptions and images, creating an engaging output. This sequential chaining ensures smooth operation while maximizing the system's ability to meet customer expectations.
"""

import os
import torch
import pandas as pd
from PIL import Image
from transformers import CLIPProcessor, CLIPModel, pipeline
import matplotlib.pyplot as plt
import random
from PIL import Image

!pip install transformers torch torchvision datasets langchain

# Move kaggle.json to the correct location
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download the dataset using the Kaggle API
!kaggle datasets download -d pes12017000148/food-ingredients-and-recipe-dataset-with-images

# Unzip the downloaded dataset
!unzip food-ingredients-and-recipe-dataset-with-images.zip -d /content/food_data

# 1. Load CLIP Model and Processor
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

"""## Read The Food Dataset"""

# Set paths to the CSV and images
csv_file = '/content/food_data/Food Ingredients and Recipe Dataset with Image Name Mapping.csv'
image_folder = '/content/food_data/Food Images/Food Images'

# Load the CSV file
df = pd.read_csv(csv_file)

# Display the dataframe to confirm it loaded correctly
df.head()

df.shape

# Path ke folder gambar
image_folder = '/content/food_data/Food Images/Food Images'

def show_random_images(image_folder, num_images=3):
    """
    Tampilkan sejumlah gambar secara acak dari folder.

    Args:
        image_folder (str): Path ke folder yang berisi gambar.
        num_images (int): Jumlah gambar yang ingin ditampilkan.
    """
    # List semua file gambar di folder
    images = [img for img in os.listdir(image_folder) if img.endswith(('.jpg', '.png', '.jpeg'))]

    if len(images) < num_images:
        print("Tidak cukup gambar di folder untuk ditampilkan.")
        return

    # Pilih gambar secara acak
    random_images = random.sample(images, num_images)

    # Plot gambar
    plt.figure(figsize=(15, 5))
    for i, img_name in enumerate(random_images):
        img_path = os.path.join(image_folder, img_name)
        image = Image.open(img_path)

        plt.subplot(1, num_images, i + 1)
        plt.imshow(image)
        plt.title(img_name, fontsize=10)
        plt.axis("off")

    plt.tight_layout()
    plt.show()

# Panggil fungsi untuk menampilkan gambar
show_random_images(image_folder, num_images=3)

"""## Download the fashion data image"""

!pip install awadb
!pip install chromadb
!pip install sentence_transformers

!pip install langchain

pip install langchain-community

import os
import shutil

# Path ke folder sumber dan tujuan
image_folder = '/content/food_data/Food Images/Food Images'
google_drive_path = '/content/drive/MyDrive/03. Work/03. IYKRA/weekly assignment - AI Engineering/week 2/food/'

# Pastikan folder tujuan ada, jika tidak buat folder tersebut
os.makedirs(google_drive_path, exist_ok=True)

# Filter file .jpg saja
jpg_images = [img for img in os.listdir(image_folder) if img.endswith('.jpg')]

# Salin file .jpg ke folder tujuan
for ind, img_name in enumerate(jpg_images):
    src_path = os.path.join(image_folder, img_name)
    dest_path = os.path.join(google_drive_path, img_name)

    # Copy file
    shutil.copy(src_path, dest_path)

    # Progress monitoring
    if ind % 100 == 0:
        print(f"Copied {ind} .jpg images")
    if ind >= 1000:  # Batasi hingga 1000 gambar
        break

print("Proses selesai!")

import uuid  # For generating unique identifiers for documents

# Import various embedding classes from LangChain
from langchain.embeddings import GPT4AllEmbeddings, HuggingFaceEmbeddings
from langchain.embeddings import AwaEmbeddings

# Import MultiVectorRetriever for managing multi-vector search and retrieval
from langchain.retrievers.multi_vector import MultiVectorRetriever

# Import Document class to structure documents with content and metadata
from langchain.schema.document import Document

# Import storage classes: LocalFileStore for local file-based storage, InMemoryStore for in-memory storage
from langchain.storage import LocalFileStore, InMemoryStore

# Import Chroma for vector storage, supporting similarity search and retrieval tasks
from langchain.vectorstores import Chroma

import ipywidgets as widgets  # For creating interactive widgets in Jupyter Notebook

# Initialize the embedding function with HuggingFace's pre-trained sentence transformer model
embedding_fn = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

embedding_fn.model_name

google_drive_path = "/content/drive/MyDrive/03. Work/03. IYKRA/weekly assignment - AI Engineering/week 2/food/"

import os  # Untuk operasi sistem
import glob  # Untuk mencocokkan pola file
import numpy as np  # Untuk operasi numerik, seperti memilih gambar secara acak
from IPython.display import display  # Untuk menampilkan hasil
import ipywidgets as widgets  # Untuk membuat widget interaktif

# Path ke folder Google Drive
google_drive_path = "/content/drive/MyDrive/03. Work/03. IYKRA/weekly assignment - AI Engineering/week 2/food/"

# Ambil semua file gambar dengan ekstensi .jpg dari folder
image_files = glob.glob(f"{google_drive_path}*.jpg")

# Pilih 5 indeks secara acak tanpa pengulangan
random_image_idx = np.random.choice(len(image_files), 5, replace=False)

# Inisialisasi list untuk menyimpan widget gambar
image_widgets = []
for ind in random_image_idx:
    # Buka setiap file gambar yang dipilih secara acak dalam mode binary read
    with open(image_files[ind], "rb") as fp:
        # Buat widget untuk gambar dan tambahkan ke list
        image_widgets.append(widgets.Image(value=fp.read(), format="jpg", width=200))

# Tampilkan gambar dalam layout horizontal
display(widgets.HBox(image_widgets))

"""## Data Ingestion"""

import base64  # Untuk encoding gambar ke format base64
import os  # Untuk operasi file
import pandas as pd  # Untuk membaca dan memproses data

# Fungsi untuk encode gambar ke base64
def encode_image(image_path):
    """
    Mengencode gambar menjadi string base64.

    Parameters:
    - image_path (str): Path file gambar.

    Returns:
    - str: String base64 dari gambar atau None jika file tidak ditemukan.
    """
    if os.path.exists(image_path):
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode("utf-8")
    return None

# Path dataset dan folder gambar
csv_file = '/content/food_data/Food Ingredients and Recipe Dataset with Image Name Mapping.csv'
image_folder = '/content/food_data/Food Images/Food Images/'

# Baca dataset dan ganti kolom 'unnamed' dengan 'doc_id'
df = pd.read_csv(csv_file)
df.rename(columns={"unnamed": "doc_id"}, inplace=True)

# Tambahkan kolom untuk string base64 dari gambar
df['Image_Base64'] = df['Image_Name'].apply(lambda x: encode_image(os.path.join(image_folder, x)))

# Siapkan text_docs sebagai list of dictionaries
text_docs = [
    {
        "Title": row["Title"],
        "Ingredients": row["Ingredients"],
        "Instructions": row["Instructions"],
        "Cleaned_Ingredients": row["Cleaned_Ingredients"],
        "Image_Base64": row["Image_Base64"],
    }
    for _, row in df.iterrows()
]

# Tampilkan beberapa contoh text_docs
print(text_docs[:3])

# Opsional: Simpan text_docs ke file .json untuk digunakan nanti
import json
with open("text_docs.json", "w") as json_file:
    json.dump(text_docs, json_file, indent=4)

# Path dataset dan folder gambar
csv_file = '/content/food_data/Food Ingredients and Recipe Dataset with Image Name Mapping.csv'
image_folder = '/content/food_data/Food Images/Food Images/'

# Baca dataset dan hitung jumlah baris
df = pd.read_csv(csv_file)
num_csv_rows = df.shape[0]  # Menghitung jumlah baris dalam CSV file

# Hitung jumlah file gambar di folder
num_image_files = len(os.listdir(image_folder))  # Hitung jumlah file dalam folder gambar

# Tampilkan jumlah file
print(f"Jumlah entri dalam CSV: {num_csv_rows}")
print(f"Jumlah file gambar di folder: {num_image_files}")

# This line of code creates a list of Document objects, called image_documents, where each document represents a base64-encoded image along with its associated metadata
image_documents = [
    Document(page_content=s, metadata={id_key: doc_ids[i]})
    for i, s in enumerate(img_base64_list)
]

import os
import pandas as pd
import json

# Path ke file CSV dan folder tujuan di Google Drive
csv_file = '/content/food_data/Food Ingredients and Recipe Dataset with Image Name Mapping.csv'
google_drive_path = "/content/drive/MyDrive/03. Work/03. IYKRA/weekly assignment - AI Engineering/week 2/food/"

# Pastikan folder tujuan ada
os.makedirs(google_drive_path, exist_ok=True)

# Baca data CSV
df = pd.read_csv(csv_file)

# Tambahkan kolom 'doc_id' jika belum ada
if 'doc_id' not in df.columns:
    df['doc_id'] = df.index + 1

# Konversi data ke dalam format text_docs
text_docs = [
    {
        "doc_id": row["doc_id"],
        "Title": row["Title"],
        "Ingredients": row["Ingredients"],
        "Instructions": row["Instructions"],
        "Cleaned_Ingredients": row["Cleaned_Ingredients"],
        "Image_Name": row["Image_Name"],
    }
    for _, row in df.iterrows()
]

# Path untuk file JSON output
json_output_path = os.path.join(google_drive_path, "text_docs.json")

# Simpan text_docs ke file JSON
with open(json_output_path, "w") as json_file:
    json.dump(text_docs, json_file, indent=4)

print(f"Data berhasil disimpan sebagai JSON di: {json_output_path}")

import os
import json
import base64
from PIL import Image
from io import BytesIO
from IPython.display import display

# Path ke file JSON dan folder gambar
google_drive_path = "/content/drive/MyDrive/03. Work/03. IYKRA/weekly assignment - AI Engineering/week 2/food/"
json_file_path = os.path.join(google_drive_path, "text_docs.json")
image_folder = "/content/food_data/Food Images/"

# Load text_docs dari file JSON
with open(json_file_path, "r") as json_file:
    text_docs = json.load(json_file)

# Index gambar yang ingin ditampilkan
f_index = 700

# Pastikan indeks valid
if f_index >= len(text_docs):
    raise IndexError(f"Indeks {f_index} melebihi panjang text_docs ({len(text_docs)}).")

# Ambil data gambar dari text_docs
image_name = text_docs[f_index]["Image_Name"]
image_path = os.path.join(image_folder, image_name)

# Baca dan tampilkan gambar
if os.path.exists(image_path):
    with open(image_path, "rb") as img_file:
        image_data = img_file.read()
        im = Image.open(BytesIO(image_data))
        display(im)
else:
    print(f"Gambar {image_name} tidak ditemukan di {image_folder}.")

# Tampilkan ringkasan (summary) untuk indeks ini
print("Summary Document:")
print(json.dumps(text_docs[f_index], indent=4))

"""## Build the Retriever"""

# The vectorstore to use to index the child chunks
vectorstore = Chroma(collection_name="summaries", embedding_function=embedding_fn)

# The storage layer for the parent documents
store = InMemoryStore()

retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=store,
)

import os
import pandas as pd
import json
import base64
from PIL import Image
from io import BytesIO
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.retrievers import MultiVectorRetriever
from langchain.schema.document import Document

# Path ke file CSV dan folder tujuan
csv_file = '/content/food_data/Food Ingredients and Recipe Dataset with Image Name Mapping.csv'
google_drive_path = "/content/drive/MyDrive/03. Work/03. IYKRA/weekly assignment - AI Engineering/week 2/food/"
image_folder = "/content/food_data/Food Images/"

# Baca data CSV
df = pd.read_csv(csv_file)

# Tambahkan kolom 'doc_id' jika belum ada
if 'doc_id' not in df.columns:
    df['doc_id'] = df.index + 1

# Pastikan kolom 'Ingredients' dan 'Instructions' dalam bentuk string
df['Ingredients'] = df['Ingredients'].fillna("").astype(str)
df['Instructions'] = df['Instructions'].fillna("").astype(str)

# Konversi data ke dalam format text_docs sebagai objek Document
text_docs = [
    Document(
        page_content=row["Ingredients"] + "\n" + row["Instructions"],  # Gabungkan Ingredients dan Instructions sebagai konten utama
        metadata={"doc_id": row["doc_id"], "Title": row["Title"], "Image_Name": row["Image_Name"]}
    )
    for _, row in df.iterrows()
]

# Path untuk menyimpan text_docs sebagai JSON
json_output_path = os.path.join(google_drive_path, "text_docs.json")

# Simpan text_docs ke file JSON
with open(json_output_path, "w") as json_file:
    json.dump([doc.metadata for doc in text_docs], json_file, indent=4)

# Load embeddings function
embedding_fn = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

# Initialize vector store (Chroma) and document store (InMemoryStore)
vectorstore = Chroma(collection_name="food_recipes", embedding_function=embedding_fn)
docstore = InMemoryStore()

# Menambahkan dokumen teks ke dalam vectorstore
vectorstore.add_documents(text_docs)

# Menambahkan dokumen gambar ke dalam docstore
image_documents = []
for doc in text_docs:
    image_name = doc.metadata["Image_Name"]
    image_path = os.path.join(image_folder, image_name)

    # Baca dan encode gambar ke dalam base64
    if os.path.exists(image_path):
        with open(image_path, "rb") as img_file:
            image_data = img_file.read()
            image_base64 = base64.b64encode(image_data).decode('utf-8')
            image_documents.append(Document(page_content=image_base64, metadata={"doc_id": doc.metadata["doc_id"]}))
    else:
        print(f"Gambar {image_name} tidak ditemukan di {image_folder}.")

# Menambahkan dokumen gambar ke dalam docstore
docstore.mset([(doc.metadata["doc_id"], image_doc) for doc, image_doc in zip(text_docs, image_documents)])

# Membuat retriever untuk pencarian berbasis teks dan gambar
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=docstore,
    id_key="doc_id"
)

print(f"Data teks berhasil disimpan di vectorstore dan gambar di docstore.")

retriever.vectorstore.get(where={"doc_id": "1345"})

"""##Test Retriever - Vector Similarity Search between Query and Text Description"""

# Define a function to display images with optional text descriptions
def show_images(docs, show_text=False):
    # If show_text is True, display the text description for each document
    if show_text:
        for i, sample_doc in enumerate(docs):
            # Retrieve the text description from the vector store based on the document ID
            text_desc = retriever.vectorstore.get(
                where={"doc_id": sample_doc.metadata["doc_id"]}, include=["documents"]
            )["documents"][0]
            print(text_desc)  # Print the description to the console

    # Create image widgets for each document to display in a horizontal layout
    image_widgets = [
        widgets.Image(
            value=base64.b64decode(sample_doc.page_content),  # Decode base64 image data
            format="jpg",  # Specify the image format as PNG
            width=300  # Set the display width of each image
        )
        for sample_doc in docs
    ]

    # Display the images in a horizontal box layout in the notebook
    display(widgets.HBox(image_widgets))

# Examples of possible queries for the retriever
# retriever.vectorstore.similarity_search("floor length gown")

# Example user queries for retrieving documents based on clothing style and occasion
query = "Suggest sweets baked fresh daily"
# query = "Suggest strapless floor length gowns that I can wear in my friend's wedding"
#query = "Suggest a dress with flowing style having floral pattern"

# Retrieve documents relevant to the query
docs = retriever.get_relevant_documents(query)

# Display the retrieved images with the function, without showing text descriptions
show_images(docs)

query = "Are these sweets baked fresh daily"
# Retrieve documents relevant to the query
docs = retriever.get_relevant_documents(query)

# Display the retrieved images with the function, without showing text descriptions
show_images(docs)

retriever.vectorstore.similarity_search("Crispy Salt and Pepper Potatoes")

"""##Enhance Multimodal RAG - with LLM Chaining"""

import re  # For regular expressions to validate base64 format

# Import necessary classes from LangChain
from langchain.schema import Document  # To define document structure
from langchain.schema.runnable import RunnableLambda  # For creating lambda functions in LangChain
# https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html
from langchain.chat_models import ChatOpenAI  # For using OpenAI chat models

# Function to check if a string appears to be in base64 format
def looks_like_base64(sb):
    """
    Checks if the given string matches the pattern of base64 encoding.

    Parameters:
    - sb (str): The string to check.

    Returns:
    - bool: True if the string appears to be base64, False otherwise.
    """
    return re.match("^[A-Za-z0-9+/]+[=]{0,2}$", sb) is not None

# Function to verify if the base64 data represents an image by examining its header signature
def is_image_data(b64data):
    """
    Checks if base64-encoded data represents an image by comparing its header to known image file signatures.

    Parameters:
    - b64data (str): The base64-encoded data to check.

    Returns:
    - bool: True if the data matches a known image format, False otherwise.
    """
    image_signatures = {
        b"\xFF\xD8\xFF": "jpg",  # JPEG signature
        b"\x89\x50\x4E\x47\x0D\x0A\x1A\x0A": "png",  # PNG signature
        b"\x47\x49\x46\x38": "gif",  # GIF signature
        b"\x52\x49\x46\x46": "webp",  # WebP signature
    }
    try:
        header = base64.b64decode(b64data)[:8]  # Decode the first 8 bytes to check the header
        for sig, format in image_signatures.items():
            if header.startswith(sig):
                return True  # Matches an image signature
        return False
    except Exception:
        return False  # Return False if decoding or header check fails

# Function to fetch matching texts and images from documents using metadata (doc_id)
def fetch_texts(docs, top_k=3):
    """
    Retrieves text descriptions and base64-encoded images from documents.

    Parameters:
    - docs (list): List of Document objects.
    - top_k (int): The maximum number of items to retrieve for each type (images and texts).

    Returns:
    - dict: A dictionary with "images" and "texts" keys containing up to top_k base64-encoded images and text descriptions.
    """
    b64_images = []  # List to store base64-encoded images
    texts = []  # List to store text descriptions

    for doc in docs:
        # Retrieve the text description based on the document's ID
        doc_id = doc.metadata["doc_id"]
        text = retriever.vectorstore.get(
            where={"doc_id": doc_id}, include=["documents"]
        )["documents"][0]  # Get the first document matching the ID
        texts.append(text)  # Add the text description to the texts list

        # Check if the document content is base64-encoded image data
        doc = doc.page_content
        if looks_like_base64(doc) and is_image_data(doc):
            b64_images.append(doc)  # Add the image data if it matches an image format

    # Return the top_k items for both images and texts
    return {"images": b64_images[:top_k], "texts": texts[:top_k]}

"""##Define the prompt"""

import os

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = ""

# Function to generate prompt messages for a multi-modal model
def img_prompt_func(data_dict):
    # Initialize an empty list to hold messages
    messages = []

    # Define the initial text message providing context and the user's question
    text_message = {
        "type": "text",
        "text": (
            "You are a friendly restaurant assistant here to help you find the perfect meal.\n"
            "Browse through a selection of delicious dishes, complete with images and descriptions, and let me help you choose your next favorite meal!.\n"
            "Use this information to help you discover the perfect dish for your meal. I'm here to guide you through our menu and find something you'll love!.\n"
            f"User-provided question: {data_dict['question']}\n\n"
            "Images and their descriptions:\n"
        ),
    }
    messages.append(text_message)  # Add the initial context message to messages list

    # Get the number of text descriptions available in the context data
    num_docs = len(data_dict["context"]["texts"])

    # Loop through each image and corresponding description
    for i, im in enumerate(data_dict["context"]["images"]):
        # Add each image as a base64-encoded URL to the messages list
        image_message = {
            "type": "image_url",
            "image_url": {"url": f"data:image/png;base64,{im}"},
        }
        messages.append(image_message)

        # Add the corresponding text description for each image
        desc_text = data_dict["context"]["texts"][i]
        messages.append({"type": "text", "text": desc_text})

    # Return all messages as a list wrapped in a HumanMessage object
    return [HumanMessage(content=messages)]

model = ChatOpenAI(
        temperature=0,  # Set temperature to 0 for deterministic responses
        model="gpt-4-vision-preview",  # Specify the GPT-4 Vision model
        max_tokens=1024  # Maximum tokens in the response
    )

# LangChain Expression Language
from functools import partial  # For creating partial functions with preset parameters

# Import LangChain components for building runnable chains and output parsing
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# Define a dummy processing chain using LangChain's runnable functions
dummy_chain = (
    {
        # Use the retriever to get context data, then run it through `fetch_texts` with top_k=3
        "context": retriever | RunnableLambda(partial(fetch_texts, top_k=3)),

        # Pass the user's question through without modifications
        "question": RunnablePassthrough(),
    }
    # Format the retrieved context and question into prompt messages for the model
    | RunnableLambda(img_prompt_func)
)

dummy_chain

from langchain.schema import HumanMessage

from IPython.display import display, HTML  # For displaying HTML content (e.g., images) in Jupyter notebooks

# Invoke the dummy_chain to generate suggestions based on the user's query
sample_docs = dummy_chain.invoke("Are there any sweet dishes that include nuts?")
# Alternative queries for testing
# sample_docs = dummy_chain.invoke("Suggest a bright dress with floral pattern that I can wear with a handbag and a sunglass")
# sample_docs = dummy_chain.invoke("Suggest a floor-length gown that I can wear in my friend's wedding")

# Loop through the content of the response generated by dummy_chain
for sample in sample_docs[0].content:
    # Check if the sample is a text message
    if sample["type"] == "text":
        print(sample["text"])  # Print text suggestions to the console
    else:
        # If the sample is an image, display it in the notebook
        display(HTML("<img alt='img' src='{}'>".format(sample["image_url"]["url"])))

sample_docs = dummy_chain.invoke(" recommend a dish that's perfect for a light lunch")

sample_docs

# Loop through the content of the response generated by dummy_chain
for sample in sample_docs[0].content:
    # Check if the sample is a text message
    if sample["type"] == "text":
        print(sample["text"])  # Print text suggestions to the console
    else:
        # If the sample is an image, display it in the notebook
        display(HTML("<img alt='img' src='{}'>".format(sample["image_url"]["url"])))

len(sample_docs), len(sample_docs[0].content)

"""##Create Full Chain"""

!pip install openai

model = ChatOpenAI(
    temperature=0,
    model="gpt-4o",
    max_tokens=1024,
    api_key=os.environ["OPENAI_API_KEY"],
)
top_k = 3
# RAG pipeline
chain = (
    {
        "context": retriever | RunnableLambda(partial(fetch_texts, top_k=top_k)),
        "question": RunnablePassthrough(),
    }
    | RunnableLambda(img_prompt_func)
    | model
    | StrOutputParser()
)

query = "sweet dishes that include nuts"

res = chain.invoke(query)
print(res)

images = retriever.get_relevant_documents(query)
display(
    HTML(
        "<table><tr>{}</tr></table>".format(
            "<td>{}</td>".format(
                "</td><td>".join(
                    "<img src='data:image/png;base64,{}' >".format(
                        sample_doc.page_content
                    )
                    for sample_doc in images[:top_k]
                )
            )
        )
    )
)

query = "some of the spiciest dishes"

res = chain.invoke(query)

print(res)
images = retriever.get_relevant_documents(query)
display(
    HTML(
        "<table><tr>{}</tr></table>".format(
            "<td>{}</td>".format(
                "</td><td>".join(
                    "<img src='data:image/png;base64,{}' >".format(
                        sample_doc.page_content
                    )
                    for sample_doc in images[:top_k]
                )
            )
        )
    )
)

from IPython.display import display, HTML
import os
import base64

query = "some of the spiciest dishes"

# Memanggil chain untuk mendapatkan hasil pencarian
res = chain.invoke(query)
print(res)

# Path ke direktori gambar
google_drive_path = "/content/drive/MyDrive/03. Work/03. IYKRA/weekly assignment - AI Engineering/week 2/food/"

# Menyimpan gambar yang relevan dari folder
images_html = ""
top_k = 5  # Jumlah maksimum gambar yang ingin ditampilkan

for i, filename in enumerate(os.listdir(google_drive_path)):
    if i >= top_k:  # Batas jumlah gambar yang akan ditampilkan
        break
    file_path = os.path.join(google_drive_path, filename)
    with open(file_path, "rb") as image_file:
        # Mengonversi gambar menjadi Base64 untuk HTML
        encoded_image = base64.b64encode(image_file.read()).decode('utf-8')
        # Menyusun HTML untuk gambar
        images_html += f"<td><img src='data:image/png;base64,{encoded_image}' style='max-width:200px;'></td>"

# Menampilkan hasil pencarian dan gambar dalam tabel HTML
display(
    HTML(
        f"""
        <table>
            <tr>
                {images_html}
            </tr>
        </table>
        """
    )
)

from IPython.display import display, HTML
import os
import base64

query = "Are there any dishes that are kid-friendly or less spicy?"

# Memanggil chain untuk mendapatkan hasil pencarian
res = chain.invoke(query)
print(res)

# Path ke direktori gambar
google_drive_path = "/content/drive/MyDrive/03. Work/03. IYKRA/weekly assignment - AI Engineering/week 2/food/"

# Menyimpan gambar yang relevan dari folder
images_html = ""
top_k = 5  # Jumlah maksimum gambar yang ingin ditampilkan

for i, filename in enumerate(os.listdir(google_drive_path)):
    if i >= top_k:  # Batas jumlah gambar yang akan ditampilkan
        break
    file_path = os.path.join(google_drive_path, filename)
    with open(file_path, "rb") as image_file:
        # Mengonversi gambar menjadi Base64 untuk HTML
        encoded_image = base64.b64encode(image_file.read()).decode('utf-8')
        # Menyusun HTML untuk gambar
        images_html += f"<td><img src='data:image/png;base64,{encoded_image}' style='max-width:200px;'></td>"

# Menampilkan hasil pencarian dan gambar dalam tabel HTML
display(
    HTML(
        f"""
        <table>
            <tr>
                {images_html}
            </tr>
        </table>
        """
    )
)